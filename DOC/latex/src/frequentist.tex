
\section{Introduction\label{sec:Intro}}

In a binary experiment, each trial has only two possible outcomes, a success(1) or failure(0).

A population being experimented on has an intrinsic property $\theta$ which is the likelihood 
that they perform the trial successfully. As a result, a test is intended to measure this
likelihood so that business decisions can be made based on its value.

In an A/B test there are two populations or branches, each of which is exposed to a different 
treatment. One branch is called the control, denoted in this article by A, which is usually a standard or baseline, 
and another is called the variant, denoted by B. The treatments could be different advertising campaigns, 
different form or content of a landing page on a website or different product offerings.

As discussed above, the populations have intrinsic success probabilities $\theta_a$ and $\theta_b$. 
The goal of this analysis is to provide a reccomendation about choosing A or B based on identitified 
business needs, using the data and justified by the statistics. 

Of course we have no idea what these intrinsic likelihoods are so we have to estimate them by exposing $n_a$ users
to our control branch A and expose $n_b$ users to our variant branch B and record how many of them actually
did what we wanted (bought a product, spent more than 10 min on a page, clicked the "register now" button, etc.)
we will represent this as $m_a$ and $m_b$ for each branch respectively. The fraction of people who did what we wanted
for each branch will be the following: 

\beq
\label{pa}
p_a = \frac{m_a}{n_a}
\eeq

\beq
p_b = \frac{m_b}{n_b}
\eeq

$p_a$ and $p_b$ are essentially our emperical estimates of the true likelihood of our success probabilities $\theta_a$ and $\theta_b$. 

In an AB Test, the implicit hypothesis is "B is better than A". However, this hypothesis is really
vaugue. What does "better" mean? Are we trying to look at the difference between probabilities? Are we looking at percent increases (lift)? 
There are an immesearable number of possible metrics and one needs to pick the right metric that uses these emperical estimates that can both 
be used in a way that not only aligns with business goals but also can be used in a way that makes sense in a hypothesis test. 
In this document we want to discuss various methodologies regaurding hypothesis tests as well as outline three different metrics we have 
identified to fit the criteria discussed above. Ultimately, we want to use these metrics to compare and contrast these various schools of 
thought as a way to give you an idea about the different stories you can tell with your data.

\section{The Frequentist Approach\label{sec:freq}}

\subsection{A Primer on Hypothesis Testing}

Before we start getting into the different ways of evaluating ab test results from a frequentist perspective, I want to spend some time building 
towards a meta understanding of freqeuntist experiment design. When setting up our experiments we have 
some intuition about what evidence we expect to see if a particular hypothesis were to be 
true. For example, if a coin is heavily weighted to favor tails then you would expect to see 
a lot of tails showing up if you kept flipping that weighted coin. 
However, if we don’t know anything about the coin, how much evidence do we need to support 
the hypothesis that the coin is not fair (\(H1\)) as supposed to being fair (\(H0\))? 
One way could be to determine a variable \(S\) which is equal to the sum of \(n\) Bernoulli 
indicators (1 if Heads, 0 if tails). We know that the expected value of \(S\) when the coin is fair should be \(0.5n\).
As a result, we could look at the deviation of \(S\) from \(0.5n\): \(|S-0.5n|\) which we will call \(D\). 
Now it makes sense to say that if \(D\) was large enough then we can say start to see evidence of an unfair coin. 
In other words, we should pick a threshold $\epsilon$ such that if \(D\) $>$ $\epsilon$ then we have enough evidence to believe
that we have an unfair coin. But now youu are probably asking, how large does $\epsilon$ need to be for me
to be confident that I have enough evidence? Let's consider the tradeoffs if we pick $\epsilon$ arbitrarily. 
If we set $\epsilon$ equal to 2, then we would probably seem extremely paranoid because we were jumping to conclusions with little information. However, if we set $\epsilon$ to 100000 then we would probably be super gullible and
potentially lose a ton of money because we were exceedingly set in our ways. In other words, we want to think about our decision of $\epsilon$ in terms
of its tradeoff with our false alarm rate or Type 1 Error. This is tricky because in most cases, you will see
that a tradeoff since being less trigger happy means being more prone to inaction. As a result, we want to pick
a false alarm rate $\alpha$ and then calculate an $\epsilon$ such that the probability of seeing D$>$$\epsilon$ 
given that the null hypothesis is true is $\alpha$. Formally, this means we want to pick $\epsilon$ such that 
P( \(D\)$>$$\epsilon$ $|$ \(H0\)) = $\alpha$. What this means is that when we collect data and get a statistic D, 
if D is greater than your chosen epsilon then there is an $\alpha$ percent chance that your trial could have been 
a false alarm. In less theoretical terminology, if your $\alpha$ was 0.05 and you performed 1000 coin flips 100 times
5 of those experiments would yield results which would cause you to accuse your friend of cheating.

Now we want to concern ourselves with the problem of picking the right $\epsilon$ such that P(\(D\)$>$$\epsilon$ $|$ \(H0\)) = $\alpha$.
Since we are going to start doing some math, lets get rid of \(D\) and go back to our old notation which was \(|S-0.5n|\).
\beq
\label{eq:confidence_intervals}
P(|S-0.5n|>\epsilon | H0) = \alpha
\eeq
when the inequality is expressed in terms of S. 
\beq
\label{eq:confidence_intervals2}
P(\epsilon-0.5n < S <\epsilon+0.5n | H0) = 1-\alpha
\eeq
Now this is a bit confusing so we can use the fact that if p=0.5 then a binomial distribution is symetric for all n and k. 
Since we are evaluating this probability given that the null hypothesis is true, we can express the above as follows:
\beq
\label{eq:confidence_intervals3}
P(S < 0.5n-\epsilon | H0) = \alpha/2
\eeq
Finally, you can look at the cumulative distribution function (BCDF) of a binomial distribution and find 
the value X such that BCDF(X) = $\alpha$/2. As a result, your $\epsilon$ is 0.5n+X.

\subsection{Neyman Pearson Hypothesis Testing}

Hopefully the previous section gave you some intuition into the inner workings of a binary 
hypothesis test. Now I am going to introduce a meta framework for approaching hypothesis 
tests and then map our example to parts of this framework.\\

Before Data is Collected:

\begin{enumerate}
\item Define your null hypothesis (H0) and your alternative hypothesis (H1).
\begin{itemize}
	\item \textit{Our null hypothesis (H0) is "the coin is fair".}
	\item \textit{Our alternative hypothesis (H1) is "the coin is not fair".}
\end{itemize}
\item Choose a statistic S which summarizes the data to be obtained. This is a function H:$\Re^n$$\rightarrow$$\Re$) such that S = H($X_1$,$X_2$,....,$X_n$).
\begin{itemize}
	\item \textit{Our statistic S is the sum of bernoulli indicator variables.}
    \item \textit{S = $\sum\limits_{i=1}^n X_i$}
\end{itemize}
\item Determine the shape of the rejection region by specifying the set of S values for which H0 will be rejected as a function of $\epsilon$.
\begin{itemize}
	\item \textit{The shape of our rejection region is $|S-0.5n|>e$.}
\end{itemize}
\item Choose a false alarm rate $\alpha$.
\begin{itemize}
	\item \textit{Our false alarm rate is 0.05.}
\end{itemize}
\item Choose this critical value $\epsilon$ such that the probability S>$\epsilon$ is $\alpha$ given the fact that the null hypothesis is true.
\begin{itemize}
	\item \textit{As shown above. Using the BCDF, find a value X such that BCDF(X) = $\alpha$/2, then your epsilon is 0.5n+X. If we perform 1000 trials, then your epsilon is 31.}
\end{itemize}
\end{enumerate}

After Data is Collected

\begin{enumerate}
\item Calculate S.
\item Reject H0 if S is in rejection region
\end{enumerate}

So now that we have setup our experiment, let’s say we perform 1000 trials 
and we see 999 heads so D=499 which is way greater than 31. This means that we can reject the null hypothesis because D is way past our epsilon. 
This is the Neyman Pearson method of hypothesis testing which requires a lot of prior thought and the choosing of a false rejection rate 
before performing your experiment. A lot of modern statistics is used to the Fischer method of hypothesis testing which is to observe data 
and then figure out your false alarm rate and gauge whether it is good enough (IE, is my p-value less than 0.05?). 
This is subject to a lot of debate, but the main point of criticism of the Neyman Pearson method of hypothesis testing from the Fisher school 
is that the choice of a false rejection rate is an arbitrary one (IE there is no actionable difference between 0.05 and 0.06) which means you 
end up rejecting more hypotheses due to being unnecessarily cautious. As a result, the fisher exact tests and other methods have been 
popularized in modern statistics, especially in biological sciences which tend to have to test hypotheses with small sample sizes (n~=20). 
However, in the world of software where data literally grows on trees, too often do I see people p-hacking their tests and picking and choosing 
statistics as a way to justify their world view rather than believing that there is a good chance that their hypothesis is incorrect. 
In the next section, I am going to setup the AB Testing hypothesis testing problem and then walk you through the Neyman Pearson method 
of evaluating that test.

\subsection{Applying Neyman Pearson Hypothesis Testing to AB Testing}

In our hypothetical example, the buisness has been having a hard time with clients bouncing from a page
that drives customers to a lot of monetizing pages so we have been tasked with decreasing 
the number of bounces on that page. Let's say we come up with a variant that accordions away some of the copy
making the CTA's a lot more prevelant. Internally we believe that this new variant page (B) will inherently reduce the
bounce rate relative to our control page (A). For now, we will quantify "reduce" via a difference metric. In less abstract
terms, our hypothesis is that the true likelihood of a user not bouncing in our new variant ($\theta_B$)
is greater than that of the control ($\theta_A$). Formally, we have the following hypotheses:

\beq
    H_0 : \theta_A=\theta_B
\eeq
\beq
   H_A: \theta_B - \theta_A > 0
\eeq

The evidence we will be collecting is a series of bernoulli trials for each page. For each page, let's perform $n_A$
and $n_B$ trials on each variant and let $A_i$ and $B_i$ be the result of the i'th bernoulli trial where $A_i$ and $B_i$
is 1 if the user doesn't bounce and 0 if the user does. Keeping with the above framework, we now want to choose 
the statistics that both summarize our data and can be used to test the hypotheses we outlined above. Just like 
we discussed in the intro, the sum of these indicators will be summarized into $m_A$ and $m_B$. These variables are discussed below:

\beq
    m_A := Bin(n_A,\theta_A)
\eeq
\beq
   m_B := Bin(n_B,\theta_B)
\eeq

However, we want to use our data to estimate the true likelihood of boucing of users exposed to the control page ($\theta_A$) 
relative to the variant page ($\theta_B$). As a result, we need to normalize $m_A$ and $m_B$ by the total number of trials $n_A$
and $n_B$. This gives us $p_A$ and $p_B$ which we defined in the intro as the sample estimates of $\theta_A$ and $\theta_B$ 
respectively. Since we are looking at the difference metric, let's define our statistic S = $p_B$ - $p_A$. A general rule 
of thumb is to pick $\alpha$ = 0.05. Given this, we want to find some threshold $\epsilon$ such that if S > $\epsilon$ then
the difference is big enough for us to reject the null hypothesis with a 0.05 chance that we are wrong. As a result, we need to
evaluate the following for $\epsilon$: 

\beq
P( p_B - p_A > \epsilon | H_0 ) = 0.05
\eeq

If you evaluate the left side of the equation. You basically want to find the area under the PDF of S [ f(S) ]such that it's equal to 0.05. 
In other words, you want to evaluate the following for $\epsilon$. 

\beq
\int_{\epsilon}^{\infty} f(S) ds = 0.05
\eeq

Now the issue is that f(S) is a scaled binomial distribution. It's integral, unfortunately, has no real closed form solution so solving this equation for
$\epsilon$ can really only be done via monte carlo methods, or we can make simplifying assumptions about the pdf of S such that its integral can be evaluated.

Luckily, a popular result is that when $n > 100$, the binomial distribution converges towards the normal distribution and so does the scaled binomial distribution.
Given this assumption, we can use a well known fact that if S is a normally distributed variable, then S/$\sigma_S$ is a standard normal variable where $\sigma_S$ 
is the standard deviation of S. Now the question is what is $\sigma_S$ given that the null hypothesis is true (worth mentioning because this comes in handy later)? 
Let's do some math to derive a solid estimate. 

\beq
var(S | H_0) = var(p_A - p_B | H_0) = var(p_A | H_0) + var(p_B | H_0) 
\eeq
Now these variance equations are defined seperately as: 

\beq
var(p_A | H_0) = \frac{\theta_A(1-\theta_A)}{n_A}
\eeq

\beq
var(p_B | H_0) = \frac{\theta_B(1-\theta_B)}{n_B}
\eeq

Putting all this together we get:

\beq
var(S | H_0) = \frac{\theta_A(1-\theta_A)}{n_A} + \frac{\theta_B(1-\theta_B)}{n_B}
\eeq

Now we use the fact that if the null hypothesis is true, then $\theta_A$ = $\theta_B$. We will denote this common
value as $\theta$. As a result, our equation simplifies down to:

\beq
var(S | H_0) = \theta(1-\theta) [\frac{1}{n_A} + \frac{1}{n_B}]
\eeq

Now because we have no idea what $\theta$ truly is, we want to estimate it with the data we have. If the null hypothesis is
true, then all of our samples must come from the same distribution which means that our sample mean of this common distribution
($\hat{\theta}$) is:

\beq
\hat{\theta} = \frac{m_A + m_B}{n_A + n_B}
\eeq

Finally, we can derive our sample estimate for the standard deviation of S ($\hat{\sigma_S}$) is:

\beq
\hat{\sigma_S} = \hat{\theta}(1-\hat{\theta}) [\frac{1}{n_A} + \frac{1}{n_B}]
\eeq

Finally, we can define our statistic as a standard normally distributed variable for large sample sizes.
Now we can solve for $\epsilon$:

\beq
P ( \frac{p_B-p_A}{\hat{\sigma_S}} > \epsilon | H_0 ) = 0.05
\eeq

We can solve this by looking at the CDF of a standard normal distribution similar to what we did above for the coin example.
I did that for you and we can say that if S is a normal (doesn't) distribution, then our value of $\epsilon$ is 1.96. 

Once we have calculated our $\epsilon$, then we can start to collect evidence and if our sample statistic turns out to be greater
than our threshold $\epsilon$ then we can reject our null hypothesis and claim that there is sufficient evidence to believe that
our alternative hypothesis is true with a 5 percent risk of false alarm. Now that we understand how to evaluate ab testing results
with the difference metric and the Neyman Pearson style of hypothesis testing, we now want to answer what the ideal sample size is 
for our test. This will bring us into the next section where we stray from our false alarm rate and start thinking about statistical
power.

\subsection{Discussion: Thinking about hypothesis test results}

Before we start talking about sample sizes and statistical power, I want us to start thinking about the many ways we could interpret 
the results we discussed above. In the above section, we discussed looking for a statistic S to encapsulate the data we were
going to observe and then draw a decision boundary $\epsilon$ in order to achieve a certain false alarm rate (0.05). To reiterate, 
we declared:

\beq
S = \frac{p_B-p_A}{\hat{\sigma}}
\eeq 

which we claimed, for n > 100, approached a standard normal distribution with mean 0 and standard deviation 1. 
When we picked an $\epsilon$ we said that if our sample S exceeded that epsilon then we could say that there was a 5 percent chance 
that the null hypothesis was true when we accepted the alternative hypothesis. The reason we could say this is because our choice
of $\epsilon$ (1.96) gives us a threshold where smaller values of S have a 95 percent chance of appearing so if we observe a value of S that's greater 
than this $\epsilon$ then we observed a value of S that is 1.96 standard deviations from our expected value of 0. Keeping in mind that our distribution 
of S is conditioned on the null hypothesis being true, observing such an extreme value of S either means that our measurement 
was a freak accident that has a 5\% chance of occuring, or that the underlying distribution was wrong. Mathematically, all this follows from the following:

\beq 
P(\frac{p_B-p_A}{\hat{\sigma}} < \epsilon | H0) = \alpha
\eeq

being rewritten as 

\beq 
P(p_B-p_A < \epsilon\hat{\sigma} | H0) = \alpha
\eeq

While the first statement thinks about about your test in terms of a critical threshold value, the second statement is
now thinking about your decision to reject the null hypothesis in terms of how likely you observe your effect size ($p_B-p_A$) 
deviate from the mean where the null hypothesis is true (expected value is 0). If your effect size deviates from 0 by $\epsilon$
standard deviations then it is likely that either our alternative hypothesis was right or that the entire methodology is wrong because S 
really didn't belong to a standard normal distribution. To confirm that the later is probably not right, you could perform the experiment 
multiple times. If you find that ~5\% of the time you are getting a false alarm then it's likely true that your alternative hypothesis was 
right. These types of metaanalyses are important especially in medicine where many peer reviewed publications require experiments be repeated 
three times with the same conclusion before being published.

\subsection{Statistical Power and Choosing the ideal sample size} 

In the previous section we made some headway around understanding the Neyman Pearson style of hyothesis testing and how it applies
to AB Testing. However, you may have noticed that I was doing a ton of handwaving around sample sizes. In software, fast decision 
making is an integral part of beating your competitors. While statistically, collecting more data will likely mean your estimate is
closer to reality, time is often a constraining factor which means that we actually want to know the minimum amount of samples necessary
to make a decision. Moreover, we also want our time investment to be worth it. Let's say you perform user studies and they 
indicate that your variant is a winner meaning you would expect your effect size to be huge. As a result, 
you shouldn't really need tons of data to proove that effect. Conversely, if you expect S to be tiny, you will probably need a ton
of data to demonstrate that effect. However, let's say you have no expected idea of how your variant will perform over the control. In that
case, you are not trying to test the hypothesis that variant B is better than variant A, you are actually trying to see whether or not it is even 
worth it to run the test. Let's say your manager is one of those "growth haxors" and wants to get huge wins quickly on a certain page that tends to get 10 samples
per day. If you found that you needed 2000 samples (200 days) to get a effect size of 0.07, that test might not be worth it for you depending on your metric. 
However, if you are running a test on a high traffic page which gets 1000 samples a day, then that test might be okay for you. Ultimately, we want to make 
sure that we have enough users to make sure that our test both has a low risk of false alarm and a high risk of detecting an effect if it exists (power).

As a result, we need to think about a second type of error which is the false negative
or Type II error rate. Recall above that the false alarm rate $\alpha$ describes the probability we accept the alternative hypothesis when the
null hypothesis was true. While we want to make sure that we minimize this, we also want to make sure that we are aware of the risk of
failing to reject the null hypothesis when the alternative hypothesis was true. The first statement is called the false negative rate which we
denote as $\beta$. However, this is confusing so we use statistical power which we denote as $1-\beta$. This is the probability we correctly reject the null 
hypothesis when the alternative hypothesis was true. Now that all of the lingo is calibrated let's pose the following question: "If we suspect an effect size 
($\theta_B-\theta_A$) of X, how large does n have to be for us to achieve $\alpha$ level statistical significance with power $1-\beta$." 
We can define the power as the following: 

\beq
P(p_B-p_A > \delta\sigma | H1) = 1-\beta
\eeq

And we can define our false alarm rate as the following: 

\beq
P(p_B-p_A > \epsilon\sigma | H0) = \alpha
\eeq

Now the reason why I brought up "suspected effect size" ( $\widetilde{S}$ ) is because we need to bring these two equations together
by envision two distributions. The first distribution is $p_B-p_A$ in the world where the null hypothesis is true which is a normal 
distribution with mean 0 and standard deviation $\sigma$. The second distribution is $p_B-p_A$ in the world where the alternative 
hypothesis is true which is a normal distribution with mean $\widetilde{S}$ and the same standard deviation $\sigma$. Now recall our
discussion of confidence intervals. Now let's say that we have to provide guarantees of $\alpha$ and $\beta$. 
If the null hypothesis was true, we need to pick an $\epsilon$ such that the probability of false alarm is $\alpha$ and pick 
a $\delta$ such that the probability we correctly detect an effect of at least $\widetilde{S}$ is 1-$\beta$. Now if we wanted to prove 
that the real effect size is $\widetilde{S}$ with our false alarm and power performance gurantees then we would use the cdf of a standard 
normal distribution and discover that we need to observe a value of $p_B-p_A$ that is $\epsilon$ standard deviations above 0 (in a world where 
the null hypothesis was true) to meet our false alarm guarantees and we would also need to observe a value that is at least $\delta$ standard 
deviations below $\widetilde{S}$ to meet our statistical power guarantee. As a result, we have the following threshold: 

\beq
0 + \epsilon\sigma = \widetilde{S} - \delta\sigma
\eeq

Now the question is what is this standard deviation. We discussed above that this was impossible to know since we didn't know what the true 
success probabilities were. As a result, we need to estimate this. To be conservative with our choice of n we need to look at the upper bound 
of our variance measure. Essentially, the wider the normal distributions are under H0 and H1 the more comfortable we can be with our choice n. 
If we let n be the total number of users exposed to both variants and we let a,b be ratios that represent the fraction of users exposed to variant
A and variant B (a+b=1) we get the following:

\beq
\sigma = \sqrt{\frac{\theta_B (1-\theta_A)}{bn} + \frac{\theta_A(1-\theta_A)}{an}}
\eeq

\beq
\sqrt{\frac{\theta_B (1-\theta_A)}{bn} + \frac{\theta_A(1-\theta_A)}{an}} < \sqrt{\frac{1}{4bn} + \frac{1}{4an}}
\eeq

\beq
\sqrt{\frac{1}{4bn} + \frac{1}{4an}} < \frac{1}{2\sqrt{n}} \sqrt{\frac{a+b}{ab}}
\eeq

Where a and b represent the fraction of people in group a and group b respectively.  Plugging this back in to the above equation, we can see 
the following:

\beq
\widetilde{S} < \frac{\epsilon+\delta}{2\sqrt{nab}}
\eeq

solving for n we get:

\beq
n < \frac{(\epsilon+\delta)^2}{4ab\widetilde{S}^2}
\eeq

As a result: a*n, b*n are the sample sizes necessary to be reasonably comfortable with your power and false alarm measurements. As a result, 
if you tell me how you plan on distributing your data (a,b), your minimum needed effect size to even consider the test worth running,
your $\alpha$ and your $\beta$. Then I will use the standard normal CDF to derive the corresponding $\epsilon$ and $\delta$ and then
use the above formula to give you the minimum necessary sample size necessary to run your experiment such that you can comfortably
reach your performance guarantees. 

Now this is probably confusing without numbers so let us add some numbers into this formula so that we can really understand this.
Let's say you are doing a 50/50 ramp ab test (a=0.5, b=0.5) and your expected effect size is 0.10 ($\widetilde{S}$ = 0.10). A standard rule of thumb
is $\alpha$=0.05 and $\beta$=0.2 which corresponds to a $\epsilon$ of 1.96 and a $\delta$ of 0.84 if you use the standard normal CDF. As a result, you 
would get: 

\beq
n < \frac{2.8^2}{4*0.01*0.25} = 784
\eeq

This means you will need a total of 784 samples with 392 coming from variant A and 392 coming from variant B to comfortably gurantee 
that your result will meet your performance standards.
