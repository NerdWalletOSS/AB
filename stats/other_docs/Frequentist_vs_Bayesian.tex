\documentclass[letterpaper,12pt]{article}
\usepackage{ifthen}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{helvet}
\usepackage{courier}
\usepackage{fancyheadings}
\usepackage{hyperref}
\usepackage{comment}
\pagestyle{fancy}
\usepackage{pmc}
\usepackage{graphicx}
\setlength\textwidth{6.5in}
\setlength\textheight{8.5in}
\input{../../latex/styles/ramesh_abbreviations}

\newcommand{\beq}{\begin{equation}} %% new, no conflict
\newcommand{\eeq}{\end{equation}} %% new, no conflict
\newcommand{\bdm}{\begin{displaymath}} %% new, no conflict
\newcommand{\edm}{\end{displaymath}} %% new, no conflict
% \newcommand{\reals}{{\rm I\! R}} %% new, no conflict
\newcommand{\reals}{\cal{R}} %% new, no conflict
% \newcommand{\mymean}[1]{\mu({#1})}
\newcommand{\bb}[1]{\mathbf{#1}}
\newboolean{longform}
\setboolean{longform}{false}
\newboolean{blogpost}
\setboolean{blogpost}{true}
%% Another option is \usepackage{comment}
%% \includecomment(answer} or excludecomment{answer} % then
%% \begin{answer} ... \end{answer}


%% From https://math.berkeley.edu/~gbergman/misc/hacks/langl_rangl.html
\newcommand{\langl}{\begin{picture}(4.5,7)
\put(1.1,2.5){\rotatebox{60}{\line(1,0){5.5}}}
\put(1.1,2.5){\rotatebox{300}{\line(1,0){5.5}}}
\end{picture}}

\newcommand{\rangl}{\begin{picture}(4.5,7)
\put(.9,2.5){\rotatebox{120}{\line(1,0){5.5}}}
\put(.9,2.5){\rotatebox{240}{\line(1,0){5.5}}}
\end{picture}}

\newcommand{\mymean}[1]{\ensuremath{\langl{#1}\rangl}} %% new, no conflict

\begin{document}
\title{Comparing Frequentist to Bayesian Approaches for Binary Outcome A/B Tests}
\author{Ramesh Subramonian, Ranjeet S. Tate, Michael Shire, Abhinav Singh}
\maketitle
\thispagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{}
% \cfoot{{\small NerdWallet Engineering }}
% \rfoot{{\small \thepage}}

\section{Parametric Approach}

Instead of calculating the above integrals for the Bayesian
probability distribution, one could try and approximate the required
probability \(\Pi[M(p_A, p_B)>0]\) by taking a parametric
approach. Consider again the one-dimensional case. Since the beta
distribution is "bell shaped", the parametric approach suggests
approximating the beta distribution as a normal distribution with
suitable mean and variance (the parameters). If we are interested in
simply knowing whether \(x-x_c>0\), we can calculate the corresponding
\bdm
z = \frac{<x>-x_c}{SE(<x>)}
\edm
and use the normal \(z\rightarrow p\) calculators or
the cumulative function of
the normal distribution to calculate the credibility.

To calculate the probability that \(M(x)>0\), one can
(quite incorrectly, since non-linear transformations will not preserve
normality) assume that the \(\{M(x)\}\) are also normally
distributed. We can calculate the mean \(<M(x)>\) and the
Standard Error of the mean \(SE(<M(x)>\) of the function using
well-known approximations and proceed as before to compute \(z\) and
\(p\). Generalizing to a function \(M(x,y)\) on 2D probability space:
\beq
\begin{split}
  <M> &\approx M(<x>,<y>)\\
  var(M) &\approx \left(\frac{\partial M}{\partial x}\right)^2\cdot var(x)
  + \left(\frac{\partial M}{\partial y}\right)^2\cdot var(y)
\end{split}
\eeq
where we are assuming that \(x\) and \(y\) are independent.

What parameters should one use for the normal approximation? If we were
taking a frequentist approach we would use \(\mu =m/n\)
and \(variance = \frac{\mu\cdot(1-\mu)}{n}\), however, these are
not the correct parameters to use for approximations to the beta
distribution. The above values are the mean and variance of the mean
for the original
binary valued distribution. What we are interested in is the normal
approximation to the Bayesian posterior distribution corresponding to the
experimental outcome \( (n,m) \), which is the beta distribution. Recall that
the mean and variance for the beta distribution are
\beq
\begin{split}
  \mu' &= \frac{m+1}{n+2}\\
  variance &= \frac{\mu'\cdot (1-\mu')}{n+3}
\end{split}
\eeq

How do we compare the normal approximation to the beta distribution?
The values for the distributions themselves are not particularly useful,
one is much more
interested in the quantiles or the cumulative function. So we
should compare the cumulative functions of the beta distribution and
of the normal distribution over the range of \(p\),
\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{CF_n40_m4}
\caption{Normal and Beta Cumulative Functions \label{fig:normal_vs_betaCF}}
\end{figure}
and see whether they
are close enough by some metric. See Fig. \ref{fig:normal_vs_betaCF} in
which we've plotted the cumulative functions of both the normal and beta
distributions for \(n=40, m=4\).

What metric should we use to compare the cumulative functions?
Since the cumulative functions
are probabilities, we will run into the same issues we had with the
difference and lift metrics.  So the metric we will use is the relative odds
ratio.  Specifically, we will calculate {\it abs(log(relative odds ratio))},
and see whether this
remains below some threshold for tolerance about 0. If our tolerance for the
relative error
in the values of the cumulative functions
is approximately 10\%, then we want the abs(log(relative odds ratio)) to be
bounded by 0.1 since \({\tt lim}_{x\rightarrow 0}log(1+x)=x\).
\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{NormalVsBeta_n40_m4}
\caption{Comparing Cumulative Functions: Normal {\em not} a tolerable
approximation to \(\beta\) \label{fig:NormalVsBeta_40_4}}
\end{figure}
In Fig. \ref{fig:NormalVsBeta_40_4} we plot the absolute value of the log
of the odds ratio for the cumulative functions of the Normal and Beta
distributions for \(n=40, m=4\), the same data as before. We see that there is
no reasonable continuous range in which the normal distribution is a
good estimate of the beta distribution for such statistically small data.

Is there a regime in which the normal distribution {\em is} a good
approximation (to within 10\% of the odds) to the beta distribution? Yes,
consider the case when \(n=400, m=20\).
\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{NormalVsBeta_n400_m20}
\caption{Comparing Cumulative Functions: Range in which Normal is a
tolerable approximation to \(\beta\) \label{fig:NormalVsBeta_400_20}}
\end{figure}
The error metric is shown in Fig. \ref{fig:NormalVsBeta_400_20}.
We see that the approximation is within the tolerance of 10\% for
\(p\in[0.038,0.068]\). From the cumulative function for beta,
\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{betaCF_n400_m20}
\caption{Percentile Range in which Normal is a
tolerable approximation to \(\beta\)  \label{fig:betaCF_400_20}}
\end{figure}
shown in Fig. \ref{fig:betaCF_400_20},
one can read off the corresponding credibility range, which is \([10\%,90\%]\).
90\% is close to the minimum credibility one should require for a
business decision, so for any credibility of interest \(>90\%\) the
normal distribution is {\em not} a tolerable
approximation to the beta distribution.

From considerations like the above, we can state as a rule of thumb
that the normal
is a tolerable approximation to the beta distribution when \(n>100\)
and \(m>20\).

For a metric comparing the two Bayesian probabilities the situation is worse
due to the approximations involved in the nonlinearities as well as the fact
that any such metric ``lives'' within \({\rm I}^2\) whereas the
normal approximation could have significant support outside this region. 

Given the ease of implementation and robustness of the numerical
integrals for the exact \(\beta\) distributions, there is no real need
to take a parametric approach and use the Normal distribution.

\section{The Frequentist Approach}
\label{sec:frequentist}
The material in this section is well-known and is included for completeness
and comparison. For an utterly convincing argument that demonstrates the
superiority of the Bayesian approach without assuming any {\em a priori}
knowledge of statistics, please take the time to read
\url{https://xkcd.com/1132/}.

\subsection{Single Variant}\label{sec:frequentist1D}
Consider the situation for one variant 
with \(n\) trials indexed by \(i\) and the corresponding
outcomes \(x_i\in \{0,1\}\). Let there be \(m\) successes (\(1\))
and \(n-m\) failures (\(0\)). Then the mean or expectation of \(\{x_i\}\) is
\bdm
\mymean{x} = \mu = \frac{1}{n}\sum_i x_i = \frac{m}{n}
\edm
and the variance of the distribution is
\bdm
var(x) = \sigma^2_x = \mu\cdot(1-\mu)
\edm
where \(\sigma_x\) is the standard deviation.

The (Frequentist) mean of the distribution is considered a good estimate
of the mean of the population:
\bdm
\mu_F = \mu
\edm
Now, we are primarily interested in the variance or the
Standard Error in the estimated mean. From the definition of variance, it
is fairly easy to show that the variance of a linear combination of
{\em independent} stochastic variables is the sum of the squares of the
components:
\bdm
var(x+\alpha\cdot y) = var(x) + 2\alpha\cdot covar(x,y)+ \alpha^2\cdot var(y)
\edm
(Think of the SE as the \(L^2\) norm of the error vector under the covariance
metric.) Since the \(\{x_i\}\) are all independent, elementary algebra yields
that the variance of the mean is
\bdm
var(\mu) = var(\mymean{x}) = n\cdot\frac{var(x)}{n^2} =/
\frac{\mu\cdot(1-\mu)}{n}
\edm
and the Standard Error in the mean:
\bdm
\sigma_F = \sqrt{\frac{\mu\cdot(1-\mu)}{n}}
\edm
One then assumes that \(\mu\) is normally distributed and uses the
cumulative function of the Normal distribution with the above parameters to
calculate \(p\)-values from the \(z\)-scores.

\subsection{Mean and Variance of a Function of Stochastic Variables}\label{sec:desc_stats_function}
As before, let \(M(x,y,\delta)>0\) be the (1-sided) proposition we
want to test, where \(M\) is one of the comparison metrics defined in
Section~\ref{sec:metrics}. If we had estimates for \(\mymean{M}\) and
\(SE(M)=\sqrt{var(M)}\) then we could calculate the \(z\)-statistic
for the proposition
\beq\label{eq:zstat}
z(M)= \frac{\mymean{M}}{SE(M)}
\eeq
and proceed to calculate the parametric credibility or
\(p\)-value \footnote{\url{https://en.wikipedia.org/wiki/P-value}}.
Since at least some of the comparison metrics are {\em not}
linear in their arguments, it is possibly not as trivial to calculate
the mean and variance exactly. The following results, taken from {\tt mean\_and\_variance\_of\_function.pdf} are decent approximations.

Let \(X = \{x^i\}\) be a stochastic variable belonging to an
\(n-\)dimensional space, with mean \mymean{X}. Let \(f(X)\) be a function on
this space\footnote{For the details of the derivation of these results
  see mean\_variance\_of\_function.pdf}.
\beq\label{eq:meanoff}
\mymean{f(X)} \approx f(\mymean{X}) + \frac{1}{2}covar(x^i,x^j)\cdot f_{0,ij}
\eeq
where the sum over repeated indices is understood. In the above approximation for the mean of a function, the first term
is the function of the mean. Consider the second term. The first
multiplicand is simply the covariance matrix for the variables
\(X\). The second multiplicand is the Hessian -the matrix of mixed
second partial derivatives- of \(f\) (evaluated at \(X=\mymean{X}\)), the \(i,j-\)th component of which is
\beq
f_{0,ij} = \left. \frac{\partial}{\partial x^i}\frac{\partial}{\partial x^j}f(X)\right|_{X=\mymean{X}}
\eeq
The two matrices are
multiplied and the trace is taken to produce a scalar.

The variance of the function is given by
\beq\label{eq:varoff}
var(f(X))\approx f_{0,i}\cdot covar(x^i,x^j)\cdot f_{0,j}
\eeq
where
\bdm
f_{0,i} = \left. \frac{\partial}{\partial x^i}f(X)\right|_{X= \mymean{X}}
\edm
is the \(i-\)th component of the gradient of \(f\) w.r.t. \(X\)
evaluated at the point \(X=X_0\).

\subsection{Calculating the Frequentist Confidence Levels}
For ease of reading we will repeat the metrics here and then present
the mean and variance for each. Note that \(x=p_A\) and \(y=p_B\) are
the probabilities for the two variants. The mean \(\mymean{x}\) and
variance of the mean \(var(\mymean{x})\) for each is calculated in the
beginning of this section. Since the two variants are disjoint, the
covariance \(covar(x,y)\) is 0.

\subsubsection{z: Probability Difference}
The proposition is
\beq
M(x,y,\delta) = x-(y+\delta) > 0
\eeq
from which
\beq
\begin{split}
  \mymean{M} &=\mymean{x} - \mymean{y} - \delta \\
  var(\mymean{M}) &= var(\mymean{x})+var(\mymean{y})
\end{split}
\eeq

\subsubsection{z: Probability Lift}
The proposition is
\beq
M(x,y,\lambda) = x-(1+\lambda)*y >0
\eeq
from which
\beq
\begin{split}
  \mymean{M} &=\mymean{x} - (1+\lambda)*\mymean{y} \\
  var(\mymean{M}) &= var(\mymean{x})+(1+\lambda)^2*var(\mymean{y})
\end{split}
\eeq

\subsubsection{z: Odds Factor}
The proposition is
\beq
M(x,y,\phi) = O(x) - (\phi  \times O(y)) >0
\eeq
From Definition~\ref{def:odds}
\begin{equation}
\begin{split}
  \frac{dO}{dx} &= \frac{1}{(1-x)^2} \\
  \frac{d^2O}{dx^2} &= \frac{2}{(1-x)^3}
\end{split}
\end{equation}
which can be substituted in Equations~\ref{eq:meanoff} and ~\ref{eq:varoff} for
\begin{equation}
\begin{split}
  \mymean{M} &=O(\mymean{x}) + \frac{var(\mymean{x})}{(1-\mymean{x})^3} - \phi*\left(O(\mymean{y}) + \frac{var(\mymean{y})}{(1-\mymean{y})^3}\right) \\
  var(\mymean{M}) &= \frac{var(\mymean{x})}{(1-\mymean{x})^4} + \phi^2*\frac{var(\mymean{y})}{(1-\mymean{y})^4}
\end{split}
\end{equation}

\subsection{Comparing Bayesian and Frequentist percentiles for a single variant}\label{sec:BvsF1}
As we will show in a later post, the odds corresponding to
the percentiles calculated via the Frequentist+parametric approach
are within \(10\%\) of the Bayesian percentile odds if:
\begin{equation}
\begin{split}
  n &\gtrapprox 100\quad{\rm and}\\
  m &\in\approx (20, n-20)
\end{split}
\end{equation}

So of course, if we have ``lots'' of data the two approaches agree.
If we don't have even as little data as required above, then the
Bayesian approach, which doesn't rely on the assumption of normality,
is more reliable. But for what? You are still going to have small
confidence intervals or large \(p\)-values, so only if you are in the
business of ``0\(\sigma\)'' decision-making would you use the results
from such a small sample. Keep in mind that for very low or very high
probability phenomena, from a statistical point of view the ``bigness
of data'' is determined by the number of positives, not the number of
trials.

\subsection{Comparing Bayesian and Frequentist for Two Variants}\label{sec:BvsF2}
Analyzing the A/B situation described in Section~\ref{sec:intro}, we
want to compare \(\mu^A_F\) to \(\mu^B_F\). If the comparison metric
is linear in both, as would be the case with the difference or the
lift, the distribution of the comparison metric is also normal, as
long as the conditions of applicability of the Central Limit Theorem
still hold for each variant. However, normality is {\em not} preserved
under non-linear transfornications, for example for the odds-factor
metric discussed in Section~\ref{sec:odds_factor}

The algebra based parametric approach described in \ref{sec:desc_stats_function} has a further
problem in that it is {\em ill-defined}.
Consider the relatively simple lift (Section~\ref{sec:metric_lift}) as a
comparison metric: The proposition is that \(p_A\) exceeds \(p_B\) by a
proportion \(\lambda\) of \(p_B\). This can be cast algebraically as
\bdm
{\rm M_{Lift1} }: p_A - (1+\lambda)\cdot p_B > 0
\edm
Since it is linear, there are no approximations involved in calculating
its mean and variance and hence \(z\). However, it can also be re-cast
algebraically as
\bdm
{\rm M_{Lift2} }: \frac{p_A}{p_B} - (1+\lambda) > 0
\edm
This is a non-linear function and approximations will be required to calculate
its mean and variance. The resulting \(z\) and confidence levels
will be different for the {\em same} proposition.

Both of the above algebraic metrics represent the same geometric object,
a curve in \((p_A, p_B)\) space. In the Bayesian approach, the proposition is
cast as the calculation of
the (weighted) area under this curve, and is manifestly independent of
algebraic transformations of the comparison metric.

??We plan to discuss alternate approaches and a more
detailed comparison of parametric and non-parametric approaches in
future posts, where we will show that the confidence level calculated via parametric approaches
can deviate very significantly from the Frequentist credibility.

\subsection{Simulations}
A possible way out ---which relies neither on the CLT with its
resulting parametric approach nor on the approximations for the mean and the
variance of a non-linear function of stochastic variables (described in a
later post)--- is to simulate
the Frequentist Ensemble as per the approach described at the beginning in
Section \ref{sec:ensemble}.
This time however, for each sub-experiment \(I\) with
measurements \((n^I_A, m^I_A, n^I_B, m^I_B)\), instead of:
\be
\item calculating the
  \(\mu^I_A, \mu^I_B\) and their means and variances,
\item calculating the approximate mean and variance of the
  comparison metric \(M(\mu_A, \mu_B)\),
\item calculating \(z(M)\) and then
\item assuming normality to calculate p-values or percentiles;
\ee
we carry out the following steps:
\be
\item For each ensemble we calculate
  \bdm
  M^I = M(\mu^I_A, \mu^I_B)
  \edm
\item From the resulting {\em measured} distribution for \(M\), calculate
  the cumulative function
\item and thence the probability that \(M\) is greater
  than some threshold
  \bdm
  P(M>\lambda)
  \edm
\ee
As we can see, the result is to make measuements {\em a la} the Frequentist
Approach, but without any assumptions about the size of the experiment,
without any assumptions of normality and without using any approximations to
calculate the mean and variance of a non-linear function.  

We will describe the results of the above calculation and a comparison to the
results from the Bayesian approach in a later post.


\end{document}
